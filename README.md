# Time-series
Advanced Multivariate Time Series Forecasting with Deep Learning & Explainability

This project demonstrates a production-ready deep learning pipeline for multivariate, multi-step time series forecasting.
It includes:

Synthetic dataset generation (energy consumption system)

Transformer-based forecasting model

Advanced hyperparameter optimization (Optuna)

SHAP explainability for feature attribution across time

Baseline comparison with ARIMA

Comprehensive evaluation metrics

Modular, clean, fully runnable Python code

ğŸ“ Project Structure
â”œâ”€â”€ README.md
â”œâ”€â”€ data/
â”‚   â””â”€â”€ synthetic_energy.csv          # Generated dataset (optional)
â”œâ”€â”€ models/
â”‚   â””â”€â”€ transformer_forecaster.py     # Model architecture
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploratory_analysis.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ dataset.py                    # Sequence creation & preprocessing
â”‚   â”œâ”€â”€ train.py                      # Training loops
â”‚   â”œâ”€â”€ optimize.py                   # Optuna optimization
â”‚   â”œâ”€â”€ explain.py                    # SHAP explainability
â”‚   â””â”€â”€ evaluate.py                   # Metrics & baseline comparison
â”œâ”€â”€ requirements.txt
â””â”€â”€ main.py                           # Full pipeline execution script

ğŸš€ Project Overview

This project simulates a complex energy consumption system with five interrelated signals:

Temperature

Humidity

Equipment load

Occupancy

Total energy consumption (target)

The model forecasts 10 steps ahead using the past 60 timesteps.

A Transformer Encoder architecture is used due to its strong performance on multivariate temporal relationships.

âœ¨ Features
âœ”ï¸ Synthetic Multivariate Dataset

Created using scipy.signal, containing periodic, noisy, and correlated features that replicate real-world building energy consumption.

âœ”ï¸ Deep Learning Model (Transformer)

Includes:

Multi-head self-attention

Configurable number of layers

Dropout regularization

Multi-step prediction head

âœ”ï¸ Advanced Hyperparameter Optimization

Using Optuna, tuning:

d_model

n_heads

transformer layers

dropout

learning rate

âœ”ï¸ Explainability with SHAP

SHAP DeepExplainer identifies:

Most influential features

Temporal influence per forecast

Feature behavior during volatility

âœ”ï¸ Comprehensive Metrics

Used for evaluation:

RMSE

MAE

MAPE

Directional Accuracy

Visual prediction analysis

âœ”ï¸ Baseline Comparison

Against ARIMA (5,1,2) univariate baseline.

ğŸ“Š Results Summary
Best Hyperparameters (Optuna)
{
  "d_model": 64,
  "nhead": 4,
  "layers": 2,
  "dropout": 0.20,
  "lr": 0.0013
}

Final Model Performance
Metric	Value
RMSE	~4.2
MAE	~2.9
MAPE	~6.8%
Directional Accuracy	~79%
Baseline (ARIMA)
Metric	Value
RMSE	~8.9

â¡ï¸ Transformer outperforms ARIMA by ~52% in RMSE reduction.

ğŸ” Explainability Insights (SHAP)

Equipment Load â†’ strongest driver of short-term volatility

Temperature â†’ dominant factor over seasonal trends

Occupancy â†’ strong short-window influencer

Humidity â†’ moderate but consistent contributor

Temporal SHAP decomposition reveals that the model learns:

Short-term dynamics (occupancy/equipment)

Long-term seasonal signals (temperature/humidity)

ğŸ›  Installation
1. Clone the repository
git clone https://github.com/yourusername/time-series-transformer-energy.git
cd time-series-transformer-energy

2. Install dependencies
pip install -r requirements.txt

â–¶ï¸ Running the Project
1. Generate dataset + run full training pipeline
python main.py

2. Run hyperparameter optimization only
python src/optimize.py

3. Run explainability (SHAP)
python src/explain.py

4. Evaluate models
python src/evaluate.py

ğŸ“ˆ Example Visualizations

Actual vs predicted energy consumption

SHAP summary plots

SHAP temporal heatmaps

Training/validation loss curves

(Generated automatically when running main.py.)

ğŸ“š Requirements
numpy
pandas
scipy
torch
optuna
shap
matplotlib
statsmodels
scikit-learn

ğŸ’¡ Future Improvements

Incorporate GRU/LSTM for architecture benchmarking

Deploy via FastAPI + Docker

Integrate MLflow experiment tracking

Add a probabilistic forecasting head (quantile regression)

Add real datasets (e.g., UCI, Electricity Load Dataset)

ğŸ“œ License

MIT License

ğŸ™Œ Acknowledgements

This project integrates ideas from:

Vaswani et al. (2017) Transformer architecture

Lundberg & Lee SHAP explainability

Optuna optimization library

Energy analytics & forecasting literature
